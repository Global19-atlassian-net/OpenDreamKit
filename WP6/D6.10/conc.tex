This report summarizes the achievements in Work Package 6 over the last year of the OpenDreamKit project. The main achievements were
\begin{compactenum}
\item The re-conceptualization of ``doing mathematics'' which leads to a better understanding of the nature and intended semantics of VRE components (see Section~\ref{sec:tetrapod}).
\item The integration of (an exemplary) formal knowledge base into the MitM Ontology that provides the pivotal point for system integration and service discovery (see Section~\ref{sec:knowledge})  
\item the development of a semantic model for mathematical datasets (see Section~\ref{sec:data}) which has been used in \WPref{dksbases} in two ways: 
  \begin{compactitem}
  \item The Warwick group led a move to inventory all the data sets, and to (manually) recover their specifications at the mathematical and data base level (schema information).
    In essence this retrofits the existing LFMDB project with the a more semantic level and has led to a vastly improved and more semantic API for LMFDB (see \url{http://www.lmfdb.org/api2/}) that has recently come online.
    A \Sage interface based on API2 is currently under development.
  \item a from-scratch implementation \dmh  that is described in Section~\ref{sec:hub}.
  \end{compactitem}
\item Special and adapted search facilities for all kinds of mathematical data and VRE components; see Section~\ref{sec:software}
\item A standalone implementation of persistent memoization in Python and GAP (see \delivref{dksbases}{persistent-memoization}).
\end{compactenum}
We will go over the various parts in detail in the rest of this section:

\paragraph{Knowledge}
We have introduced an upper ontology for formal mathematical libraries (ULO), which we propose as a community standard, and we exemplified its usefulness at a large scale.
We posit ULO as an interface layer that enables a separation of concerns between library maintainers and users/application developers.
Regarding the former, we have shown how ULO data can be extracted from formal knowledge libraries such as Isabelle.
We encourage other library maintainers to build similar extractors.
Regarding the latter, we have shown how powerful, scalable applications like querying can be built with relative ease on top of ULO datasets.
We encourage other users and library-near developers to build similar ULO applications, or using future datasets provided for other libraries.

Finally, we expect our own and other researchers' applications to generate feedback on the specific design of ULO, most likely identifying various omissions and ambiguities.
We will collect these and make them available for a future release of ULO 1.0, which should culminate in a standardization process.

\paragraph{Data}
We have analyzed the state of research data in mathematics with a focus on the instantiation of the general FAIR principles to mathematical data.
Realizing FAIR mathematical data is much more difficult than for other disciplines because mathematical data is inherently complex, so much so that datasets can only be understood (both by humans or machines) if their semantics is not only evident but itself suitable for automated processing.
Thus, the accessibility of the mathematical meaning of the data in all its depth becomes a prerequisite to any strong infrastructure for FAIR mathematical data.

Based on these observations, we developed the concept of Deep FAIR research data in mathematics.
As a first step towards developing a Deep FAIR--enabling standard for mathematical datasets, we focused on relational datasets.
We presented the prototypical \dmh system that lets mathematicians integrate a dataset by specifying its semantics using a central knowledge and codec collection.
We expect that \dmh also helps alleviate the problem of \emph{disappearing datasets}:
Many datasets are created in the scope of small, underfunded or unfunded research projects, often by junior researchers or PhD students, and are often abandoned when developer change research areas or pursue a non-academic career.

\paragraph{Software: computational mathematical documents}
For the S aspect of D/K/S structures from the \pn proposal or the \textbf{narration} and \textbf{computation} aspects of the finer tetrapod model from Figure~\ref{fig:tetrapod} we have developed a formula harvester for Jupyter notebooks and a formula search engine that builds on them.

To make this possible, we had to invest a heavy dose of software engineering into the MathWebSearch system: Even though the system has successfully been used as a formula search engine in zbMATH publication information system (see \url{https://zbmath.org/formulae/}), the deployment of the system required a lot of domain-specific development and workflow integration.
To this end we have developed Go bindings for the MathWebSearch daemon, documented the interfaces, and provide a web application wrapper.
With this, specific applications only need a domain-specific harvester and front-end.
We have exercised that for the Jupyter Search engine  (as envisioned in task \taskref{dksbases}{mws}) and analogously for a formula search engine for the $n$ category Cafe (nLab, see \url{https://nlabsearch.mathweb.org/}).


\paragraph{Persistent Memoization}
This can also be seen as a source (and implementation) of mathematical datasets, but with an eye on computation rather than than semantics.
Even though it is called ``persistent'' memoization, the temporal scope of the memoized data is less than the ``eternity-scope'' of datasets in LMFDB and \dmh. Indeed, the characteristic innovation in \delivref{dksbases}{persistent-memoization} is that mathematical objects and data can be shared ``across multiple computations''.
In the use cases studied in \pn so far, the semantic level can be left implicit to the implementations, e.g. between \Sage and \GAP, and does not need a uniform search or query interface.
But it is clear that the borders between persistent memoization and mathematical datasets are fluent; indeed many datasets we have now started out as computation caches which ended up being shared in the community. 
We will study the inter-conversion of memoization caches and full datasets on \dmh in the future. 

%%% Local Variables:
%%% mode: latex
%%% mode: visual-line
%%% fill-column: 5000
%%% TeX-master: "report"
%%% End:

%  LocalWords:  standardization analyzed Realizing emph ednote summarizes re-conceptualization WPref dksbases compactitem dmh delivref textbf textbf Jupyter zbMATH taskref mws
