\documentclass{deliverablereport}


\usepackage[style=alphabetic,backend=biber]{biblatex}
\addbibresource{../../lib/kbibs/kwarc.bib}
\addbibresource{../../lib/deliverables.bib}
% \addbibresource{rest.bib}
% temporary fix due to http://tex.stackexchange.com/questions/311426/bibliography-error-use-of-blxbblverbaddi-doesnt-match-its-definition-ve
\makeatletter\def\blx@maxline{77}\makeatother

\usepackage{tikz}
\usepackage{standalone}
\usepackage[show]{ed}

\usepackage{graphicx}
\usepackage{float}

\usepackage{color}
\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}

\deliverable{dksbases}{design}
\issue{136}
\deliverydate{31/08/2016}
\duedate{31/08/2016 (Month 12)}
\def\pn{OpenDreamKit\xspace}

\author{Tom Wiesing}

\usepackage[parfill]{parskip}

\begin{document}

\maketitle\vfill

\begin{abstract}
Yet to do
\end{abstract}

\vfill

\newpage\tableofcontents\newpage

\ednote{Write an abstract}
\ednote{Update delivery data}

\section{Introduction}\label{sec:intro}

The OpenDreamKit project \cite{ODKproposal:on} is an EU Horizon 2020 project with the aim to deliver flexible mathematical toolkit for working with mathematical knowledge. To achieve this goal the project in particular wants to allow multiple mathematical knowledge systems (such as Sage, GAP, \dots) to work together\ednote{Cross-check with proposal document}.

To facilitate communication between these systems they need to have access to the same mathematical knowledge. This allows them to communicate and, for example, exchange objects between systems. By enabling exchange of objects between systems, each system can be used to compute properties of mathematical objects in the area in which it is best. We call this approach the Math-In-The-Middle approach \cite{DehKohKon:iop16} because the underlying mathematical knowledge, the ``real math'' is in betweeen -- in the ``middle'' -- of the systems. Just having access to this knowledge is not enough -- each of the systems involved has its own particularities and needs some kind of interface to this knowledge. We model this approach using the well-established framework of theory graphs.

\subsection{A Brief Recap of Theory Graphs}

A theory graph consists of theories and the relations between them. A theory in this sense is a set of declarations -- a set of declared symbols. In addition to the declarations, each theory has a name (which together with its namespace forms the global URI for the theory) and a meta-theory. A meta-theory is commonly the logical framework that is used to model the content of the theory. Each declared symbol has a name and can additionally have a type, a definition and different kinds of meta-data. In each theory these symbols can then be used to form terms that can be used to express more advanced knowledge. Here terms are effectively OpenMath 2.0 \cite{BusCapCar:2oms04} objects -- they mostly consist of literal values, symbols and applications of terms to other terms.

There are two basic kinds of relations between theories: imports and views. An import is a way to declare symbols from one theory in another theory -- to import the symbols from a source theory to a target theory. This can for example be used to extend an existing theory without re-declaring all symbols or to combine two theories. Furthermore the concept of imports allows to modularise knowledge. On top of imports there are also Structures which are imports and additional renamings of the imported symbols. The second type of relation, the view, is a mapping from one theory to another -- a way to ``view'' one theory as another. This mapping allows terms from one theory to be translated into another theory. In the case where terms represent boolean statements or proofs, the mapping given by the view should be truth preserving -- \emph{i.e.}~if a statement is true in the source theory, it should be true in the target theory after translation\ednote{Give an example theory graph here}.

Theory graphs are implemented inside the MMT system \cite{RabKoh:WSMSML13}. The system allows for the declaration of theories along with symbols, imports and views. Furthermore it is possible to create terms over these theories and translate them along views. The MMT system also provides a type checker that can be used to type check declarations.

\subsection{The Need For DKS Theories}
The OpenDreamKit project is meant to construct a modular toolbox of mathematical software. One possibility is to interface the different systems pairwise. An additional possibility is to facilitate the interactions between the various systems through a Math-in-the-Middle "backbone", which would clearly be of prime importance. 

In the OpenDreamKit project we want to use this modular approach to mathematics, and in particular the MMT system, to allow us to easily translate mathematical expressions between systems. For this purpose we define a ``Math In The Middle'' theory as well as interface theories for each system. With the help of MMT and bi-views\footnote{A bi-view is a bidirectional view between two theories} between the interface theories and the central theory, we can translate objects from one system to the other. \ednote{possible include a reference to \cite{KohManRab:aumftg13}; this probably does not fit in here well enough though}

This however is still not enough -- we need three aspects to properly model our situation. In addition to the Knowledge (Math-In-The-Middle) and Systems (Interfaces) there is also a third aspect that is involved inside these systems -- the Data aspect. Together these three form so-called \textit{DKS} theories. Thus we need a framework to allow MMT to handle big databases of knowledge. In this report we focus on our progress towards this goal.



In Section~\ref{casestudy}, we will present the different systems that are most relevant to the OpenDreamKit project, in the sense that some if not all of their components would be interfaced with others. 
In Section~\ref{sec:data} we describe in detail how we model data-centric theories, then continue in Section~\ref{sec:lmfdb} by describing and demonstrating the example we have implemented already. We conclude in Section~\ref{sec:conclusion} by summarizing what we have achieved so far and giving a short outlook on how we plan to build translations between systems.

\section{Report and Case-Study}\label{casestudy}
\ednote{Paul: Write about study}
\subsection{FindStat}
1. Overview at a high level of your system

FindStat is a database and a web interface accessing the database designed for combinatorists. 
The purpose is to store and search informations on *statistics* over *combinatorial objects*.  
A statistic is mostly a map between a set of combinatorial objects to the natural numbers. As an
example, the number of edges is a statistic on graphs. The main purpose of FindStat is to give an
interface for one to *search* for some statistics the same way one would search for integer sequences
on oeis.

1. What data do you have?
 1. Structure

We have basically 3 categories of objects.

**The combinatorial collections** FindStat stores a list of combinatorial collections: 18 as of today (January 2016).
All our combinatorial collections are actually linked to a *Sage* combinatorial collection. We store the minimal informations
for us to print the collection on our website and recreate the collection in Sage. 

For every collection, we store a list of combinatorial objects. More precisely, we use *Sage* to generate the list of objects,
but we store a standardised version of the printout of the object. This standardised version is homemade: it has to be 
* standardized: a single given graph will always be printed the same way,
* unique: two different graphs will never be printed the same way,
* human readable: when possible, it should be easy to understand for a Human reader and not only a machine (so no hashkey or anything like this).
When possible, we keep the default printout of Sage object. Sometimes, we have to store a little bit of code to convert this printout into a
Sage entry.

**The statistics** A Statistic is a list of couples : combinatorial object from a certain collection / value. As of now, we have 364 statistics, 
each of them containing between 200 and 1000 entries. For each statistic, we store some metadata: Name, identifier 
(specific to FindStat, can be referenced from outside), combinatorial collection, description, code, references, etc. And we also store the data itself: a list of entries, 
each entry is made from combinatorial object (as a string, by its standardised printout) and a integer value. As an example, the values of "The number of Edges of a graph" 
St000081 is a list of all graphs up to size 6 with their associated number of edges.

**The combinatorial maps** A combinatorial map is a mathematical function from a combinatorial collection to another combinatorial collection. For example: binary search
tree insertion turns permutations into binary trees. As of now, we store 107 maps each of them containing between 200 and 1000 entries. We store the metadata of the map: domain, codomain, description, code, etc. And we store the mapdata as a list of (value, image) 
where value and image are combinatorial objects stored as strings through their standardised printout.

As an addition, we also provide some **wiki pages** with informations on combinatorial objects, maps and statistics in a less formalized way.
 
 1. Format

Our low level data format is a SQL database where we store everything we need. Most of the data described above is accessible through the website in HTML.
All information about *statistics* and *maps* can be accessed through *json* files that have standardised url depending on the identifier of said statistics or
maps. It is possible that the url changes if the website organisation is changed in the future but it will always be related to the identifiers which are set once and for all.
The format of the json files are also likely to change but we try to limit those changes and keep backward compatibility as much as possible. Those json files
are the closest we have to an external API, their are used by the *Sage FindStat interface*.

All our data are distributed under Creative Commons Attribution-ShareAlike 3.0 Unported License. 

 1. How is it produced?
 1. How is it changed?

The data are produced and changed through user contributions. As for now, 55 people are listed as contributors. We have an HTML form to submit statistics where
the user receives many informations on what should be submitted and in what format. Once a new statistic is submitted or a change is proposed, it has to be validated
by one of the FindStat developers. We don't receive that much data so the process is usually very quick. Each change is stored and so we have access to the full history
of the statistic informations with authors.

For maps, we don't have yet the "Add Map" form. Each map has to be added by one of the FindStat developers. The reason is just that the maps are a more recent addition and
so the adding process has not been finalized yet.

 1. How do you document it?

We have a very basic documentation for statistic data that we provide to the user who which to contribute. We don't have any documentation for our dataformat (json files).

1. What knowledge do you have?
 1. Sources of external knowledge?

We relate on the knowledge of our contributors about statistics and maps and try to store it. We also depend on some Sage algorithms, for example to generate the combinatorial
objects.

 1. Can you point to implicit knowledge? Is it common knowledge?

Our website is targeted at combinatorists. Even though, we try to give all the basic definitions and information, it might be difficult to use for someone who has no
knowledge of these objects.

 1. What would you gain if it was made explicit/machine actionable?

At the moment, our infrastructure is really Sage oriented (object printouts, names, etc). A language-neutral description of our objects might make it easier for interfaces 
from other system to appear. The gain for us is that the more user we have (from different background), the more contributors we might get and so the more mathematically 
pertinent our database is.

 1. Have you gone in this direction? How did you represent the knowledge then?

Giving access to the statistics and maps data as json files was a first step in this direction. 

 1. How do you collaborate on knowledge representation? 

By referencing those data (statistics and maps) and proposing unique identifiers that can be referenced from the outside (the same way the OEIS 
identifies integer sequences with a unique number). 

1. What software do you have?
 1. What custom software are you running?

We need the software *Sage* to run some computations: basically, generating the objects, printing them, etc. The statistic and maps code are usually given into
Sage for consistency but it is not mandatory.

There is also some FindStat specific code to run the website. Most of this code is just basic webprogramming views of our database. 

The database search is the heart of the service. It is a small algorithm that takes a user given statistics and compares it to the database up to some maps.

 1. In which language?

Our server runs on *Sage* with some imported web packages, so it is written in python. We use the python wiki server *MoinMoin* as a backend 
and have written some customized *MoinMoin* pluggings to run our service.

 1. How does it use the data and the knowledge?

The data is stored in a SQL database. It is preloaded and precomputed when we launch the server then all computations are made on this preloaded data. 
We don't use the knowledge at this stage, we just basically request the database and compare numbers using some parameters. In the future, we might wanna
use the knowledge we have on the maps (bijection, injection, surjection, involution, etc) to improve our algorithm.

 1. How does your software act on represented knowledge?

The software might put into light some mathematical relations between combinatorial objects but doesn't store them or anything like this.
 
1. Mixing (revisit?)
 1. Which knowledge is implicit in the data you have?
 1. Which knowledge is implicit in the software you have?

1. Anything else?

\subsection{Sage}
  - skim over sage databases (i.e. mention they exist, not more)
  - talk about different uses of category framework, including Florent's tricks of adding new axions to check consistency
  - talk about persistence/pickling of math objects as one point where semantics are implicit
  
  - [SageMath](http://www.sagemath.org): General purpose computational (pure) mathematics software

- 300 contributors

- 1.5M lines of Python/Cython code

- 40k functions

- 4k classes

- hundreds of open source (math) software/libraries in the distribution

--

---
 What data do you have?

 A collection of (optional) databases:

- Usually coming from external software/databases

- Examples:

  - GAP databases
  - OEIS
  - John's database of elliptic curves
  - ...

---
 Pickling / Serialization

Objects can be converted to strings and reconstructed

- Applications:

  - Persistence
  - Sage databases
  - Exchange of data between Sage instances

- Format: Python's pickle protocol

  - Code to reconstruct the object + sanity checks
  - By default: pickling by class + plain data (no encapsulation)
  - Aiming for: pickling by construction (more semantic)



---
 What knowledge do you have?

Mathematical properties and theorems, algorithms, ...


 Two key points that conditioned the design

- There are only a handful of fundamental concepts:

  operations: *, +, ...

  axioms: associativity, commutativity, ...

  Richness in the combinations of them (e.g. Fields)

- Using an existing language and its object oriented features for
  modelling and method selection


 Sources of external knowledge?

Each Sage contributer brings on specific mathematical knowledge about
the objects studied, which might not be available to others in the
collaboration.

--

 Can you point to implicit knowledge?

- The algorithms rely heavily on the mathematical properties of
  the objects they manipulate.

- Sage uses the Object Oriented features of Python

  The properties of a Sage object are specified by its *class*:

  - what mathematical object does it represent?

  - how is it represented

- The class information is often of technical flavor, and complemented
  by additional information on its universe (parent, category)

--
- Sage strives to model mathematics closely:

  Not only matrices are instances of a specific classes and not plain
  list of lists

  But linear maps are instances of specific classes and not just
  represented by matrices

  ==> Reduced risk of calling a meaningless function

--

- The abstract algebraic properties of an object (e.g. being a group
  or a field) are modelled relatively explicitely:

  Objects know the names of their categories and axioms

  Meaning essentially implicit except, in the good cases, informally
  in the documentation and as testing methods

- The names of the operations are hardcoded

  ==> duplication: additive / multiplicative / lie magmas

  Morphisms by automatic renaming? Lacks static typing?

- Taming the exponential blow

  Size of the code linear in the number of methods

--
- It's not always defined explicitly which methods an object in a
  given category should implement.

  Methods/operations are documented, but their exact specifications of
  is not always completely defined/defined consistently accross the
  class hierarchy.

- Some theorems (e.g. wedderburn) are embedded in actionable form,
  but that information cannot be extracted / operated on

--
 Is it common knowledge?

The meaning of the relevant categories / axioms (e.g. ring,
associativity) is relatively well known by the users and developpers.

--
 What would you gain if it was made explicit/machine actionable?

- Dynamic generation of documentation that the user can navigate
- Sanity/correctness checks; proofs?
- Semantic handles to communicate with other systems
- Avoiding duplication (additive magmas / multiplicative magmas)?

 Have you gone in this direction? How did you represent the knowledge then?

- Categories / axioms were a first step :-)

 How do you collaborate on knowledge representation?

- Collaborative development of code / doc / tests in the Sage sources

---
 What software do you have?

 What custom software are you running?

- 1.5 M lines for the Sage library + all the rest

 In which language?

- Python/Cython + myriads of languages used by subsystems

 How does it use the data and the knowledge?

- As a fundation for its hierarchy of classes/categories

- Those are used for code factorization, documentation, and generic testing

--

 How does your software act on represented knowledge?

- Some computations in the lattices of categories:

  X is a division ring and X is a finite set

  ==> X is a finite field

---
 Mixing (revisit?)

 Which knowledge is implicit in the data you have?

 Which knowledge is implicit in the software you have?

- Formal definition of axioms

  That can be manipulated by the machine

- Formal specifications of methods

  That can be manipulated by the machine

1. Anything else?

Nope
<section>

\subsection{GAP}

\subsection{LMFDB}

1. Overview at a high level of the LMFDB system

 The [L-functions and Modular Forms DataBase](http://www.lmfdb.org) aims to aggregate and integrate computational and mathematical knowledge about L-functions and other number theoretic objects, and to present these complex and interconnected objects reliably while maintaining accessibility. At a mathematical level, this could help provide a uniform view of the concept of L-function, objects which can (sometimes conjecturally) be produced out of very different mathematical constructions. The collaboration involves around 50 mathematicians of varying coding skills and with different mathematical expertise.

1. What data do you have?

 The entirety of the data held by the LMFDB is [accessible through an API](http://www.lmfdb.org/api/). One counts around 30 different types of objects stored, for a total of a few TB. The data is downloadable  [here](http://www.lmfdb.org/data/dump/).

 1. Structure

   The data is held in a Mongo database server, holding around 30 or so databases, each with their own collections. 
 
 1. Format

   The data is held in the database as BSON (binary JSON), the internal format of Mongo documents. 
 
 1. How is it produced?

   Data that ends up in the LMFDB has many different origins. Some are historical computations. Most are done in either GAP, Pari, Sage, Magma, etc, with the person who coded these original sources a member of the LMFDB who aims to make their data more accessible to their peers. Some of the data shown on the website is actually computed on the fly. 

    Data comes in through a variety of ad hoc ways, but essentially always transits through a JSON format before upload to the Mongo database. At some point there was discussion of allowing anyone to upload their data through an online form. This option is still there, but sees little use. 
 
    In general, proper referencing of data sources and documentation of its quality is a struggle. 
 
 1. How is it changed?

   Updating is mostly done through some form of overwriting. 
 
 1. How do you document it?

   The various formats are [in the process of being formalised]
   % https://github.com/LMFDB/lmfdb-inventory
   . The most advanced example is on elliptic curves
   % https://github.com/LMFDB/lmfdb-inventory/blob/master/db-elliptic_curves.md
   . The formalisation format itself does not have a spec. 

1. What knowledge do you have?
 1. Sources of external knowledge?

   Each participant in the LMFDB brings on specific mathematical knowledge about the objects studied, which might not be available to others in the collaboration. The LMFDB has the concept of "knowls", which are encyclopedic bits of content integrated alongside the data, and editable collaboratively. 

 1. Can you point to implicit knowledge? Is it common knowledge?

   There is a lot of implicit knowledge in the encodings chosen for the data (ad hoc formats and references), some of it is made explicit (for instance [here](
   % http://www.lmfdb.org/knowledge/show/ec.conductor_label
   )). There is also a lot of implicit knowledge in the source code. There is little common knowledge across the collaboration, or at least there is a lot that is not common. 
 
 1. What would you gain if it was made explicit/machine actionable?

   I (Paul) think the development process could be made more robust and efficient. The knowls currently serve as entry points for users and crucially also for onboarding future collaborators, as a stable basis for further collaboration. LMFDB could gain in productivity, robustness and ultimately utility if this process could be extended a bit further. 
 
 1. Have you gone in this direction? How did you represent the knowledge then?

   The furthest the LMFDB has gone into the direction of formalising knowledge is in modularising as much as possible of the mathematical knowledge through knowls, creating an ad hoc ontology to classify them, and aligning it to the mathematical data objects that are presented. The LMFDB also tries to adhere to the concept of "one URL per object". 
 
 1. How do you collaborate on knowledge representation? 

   Edition of the knowls requires an account, which the LMFDB intends to offer to anyone. There is some versioning in place for knowls. 
 
1. What software do you have?

 The LMFDB is mostly written in Python, relies on Sage and Pari/GP as libraries. It uses the database MongoDB (and possibly also an SQL one), uses the web framework Flask, and the templating engine Jinja. 
 
 1. What custom software are you running?

   In a way Sage is custom, since lots of LMFDB developers also contribute to Sage. Otherwise a whole lot of the logic is embedded in the website code. 
 
 1. In which language?

   In the Flask framework. 
 
 1. How does it use the data and the knowledge?

   Generally, a URL path will be associated to a Jinja template, requiring simultaneous fetching of pre-entered knowledge (knowls, Mongo DB), precomputed data (Mongo DB), and on-the-fly computation based on this precomputed data or existing functions implemented in some of the Computer Algebra Software already used. 
 
 1. How does your software act on represented knowledge?

   As far as I know, it doesn't, except to display knowls. 
 
1. Mixing (revisit?)
 1. Which knowledge is implicit in the data you have?

   Again, a lot of the data encoding is implicit in the data. For instance, `[0,4,5,1]` might represent the polynomial $4*x+5*x^2+x^3$ or the polynomial $x(x-4)(x-5)(x-1)$.
 
 1. Which knowledge is implicit in the software you have?

   When populating templates, some of the mathematical knowledge might be really entered through the code, by completing the template in different ways according to the calling class (example: elliptic curve L-functions are of degree 2).
 
1. Anything else?

 Nope
 
Thanks for making a good start on this.  I do have some comments, some of
which reflect rather recent changes and decisions made during the
just-finished semester at ICERM which included weekly LMFDB working
sessions.

About uploading of data: "This option is still there, but sees little
use."  This has never been a realistic option, and is not even visible
unless a user of the website is a developer with an account and is logged
in.  Perhaps best to say "This option has never been used seriously, and is
not currently supported."

"In general, proper referencing of data sources and documentation of its
quality is a struggle."   Progress has been made on these through a new
collection of "data quality" knowls, see
http://www.lmfdb.org/knowledge/?category=dq .  This only started within the
last few weeks but the intention is to have source / extent / quality /
reliability all documented for the main sections of the database by next
March's workshop.  As an example, see all elliptic curve pages such as
http://www.lmfdb.org/EllipticCurve/Q/  there is a "Learn more about"
section in th euppoe right which links to these knowls.

Under "How is [the data] changed" we could say more though it is not
uniform across the LMFDB.  In the best cases, the data is stored completely
separately fom the LMFDB's own (mongo) database, e.g. in my ecdata github
repository, under full revision control of text files;  and there are
scripts to populate the d.b. from that.

The process of documenting the data formats is ongoing, as has been driven
recently by the writing of the API by Harald Schilly -- all very new.

I don't know if it is relevant here but we are also accumulating a set of
bibliographic references and have already instuted a system for making
citations within knowls very easy.

\section{Modelling data-centric theories}\label{sec:data}

To properly model data-centric theories we first need to define them. In our setting we considder a theory data-centric if most of the declarations contained within it are instances of a single concept that usually originate from a database. But what exactly does this mean? Consider the example of having a concept of sequences of natural number formalised inside a theory. In this setting we could create a data-centric theory by having the theory a theory that contains all sequences of natural numbers as concepts.

This setting is in practice very difficult for multiple reasons. Mainly, the number of declarations is infinite. This in itself is not a problem, however a theory containing all sequences is just not very useful in practice. Even if we limited ourselves to a large finite subset of sequences -- take for example thoise defined in OEIS \ednote{cite OEIS} a further problem arises: We never want to manually write a theory that contains all these declarations. Hence we need to introduce dynamic on-demand declarations inside of MMT -- declarations that are only evaluated once they are needed.

For this we want to some form of database directly from within MMT. Furthermore this need to be invisible to the user -- they should not notice the difference between an actual theory and a theory that contains virtual declarations. This of course brings with it a few disadvantages. First and foremost we need to be connected to the network in order to properly handle this theory -- a request has to be made to the database each time we look for a new declaration.

\subsection{Using Codecs To Understand Database Objects}

We also need to be able to translate between the representations of the objects stored in the database and proper MMT terms -- objects of a certain type that properly represent the corresponding mathematical objects. In most common databases, each record stored usually has multiple fields that each have values. These values are usually represented with some physical data type -- for example an integer is stored in the form of a 64-bit integer\ednote{Better example?}. In some of the cases it is trivial to translate the encoded values to the actual, intended values. In some cases, it might not be as trivial as it seems. Take for example a field with the type matrix of integers. On one hand it could be represented as a list of list of integers. On the other hand the database could use some kind of sparse representation, i.e. store only the non-zero entries. To encompass this setup inside MMT we introduce the concept of codecs.

A codec is a triple $(t, f, g)$ where $t$ is a type (represented as a term inside the Math-In-The-Middle Theory), $f$ a mapping from the encoded representations of the type to the intended values of the type and $g$ is a mapping from the actual values of the type to their physical encodings. We call the operation performed by $f$ a decoding and the operation performed by $g$ an encoding of the type $t$.

There are two basic ways of creating codecs -- atomic codecs and codec operators. An atomic codec is a codec of a very simple type, for example an encoding of integers as strings. In this case $f$ would be a map from strings consisting of digits\footnote{Technically, there may also be a starting \texttt{"-"} or \texttt{"+"} sign, but the details are not important here. } to integers. This mapping does exactly what you would expect -- it maps the string \texttt{"0"} to 0, the string \texttt{"1"} to 1, etc. $g$ would be an inverse of sorts -- the map from integers to strings that maps 0 to \texttt{"0"}, 1 to \texttt{"1"}, etc. Note that here $g$ is the left inverse of $f$ (in the sense that $g \circ f$ is the identity), but $f \circ g$ not neccessarily. In a general setting we want $g \circ f$ to be the identity for all codecs, however in practive it rarely happens. A good example of this is the problem of precision -- there is no chance of encoding all of the real numbers in a physical (countable, probably even finite) datatype.
The second way of creating codecs is by using codec operators. These take as input an atomic codec and give a codec for a composite type. Take for example the list codec operator. It takes as input a codec for an arbitary type $t$ and gives a codec for the type \texttt{List(t)}.

\ednote{Give more examples of codecs}
Inside MMT we can represent the actual values as literals. For this we do not use the representations from the database, but instead values that best represent the true values. We also represent the codecs inside MMT. Each codec can be represented as an MMT term, with each atomic codec  and each codec operator having an associated symbol inside a Codec theory We can use these symbols to represent arbitary codecs, for example a codec for a matrix of integers would be represented by applying the symbol for a matrix codec operator to the symbol for an integer codec. For each codec and codec operator we have an associated Scala class. For atomic codecs, this class implements an \texttt{encode} and \texttt{decode} method (implementing $g$ and $f$ respectively). In the case of Codec operators, it implements a \texttt{build\_codec}\ednote{Correct method name} method -- that takes as input an AtomicCodec and returns an codec for the composite type.

To be able to construct the codec that corresponds to a Term we have also implemented a Coder class\ednote{Different name?}. It starts from a term representing a codec, finds the appropriate atomic codec(s) and codec operator(s) and then uses them to construct a Codec instance. This allows us given a literal value  and a Term representing its type to encode or decode terms in almost arbitary ways.

\subsection{Using Records and Schemas inside MMT}

Now that we are able to translate between the representations of values of fields of records inside an external database we want to model the record as a whole inside MMT. For this we introduce a type of records inside MMT. This is just what you would expect -- a list of (key, value) pairs with each key appearing at most once. In ou rimplementation the keys are MMT symbols and the values are arbitary MMT values. We also introduce a projection operator. This takes an MMT record and a key and returns the values of the key in the given record.

Next we want to be able to represent the database record inside MMT. For this we assume that the records inside the database are homogeneous, that is each record has the same fields and the values have the same types. We require the values of these types to be decodable with a single codec. For this purpose we introduce the concept of a schema theory. This schema theory has a declaration for each field in the database. Furthermore we use meta-data to annotate each declaration by stating which codec it uses. Since a codec can be represented by an MMT term this is easy to implement.

The schema theory tells us exactly how a record inside the database looks like and how to turn it into an MMT term. However we also want to be able to have a type to be able to talk about the theory of all records in the database. This contains only two fixed declaration: A type for the database objects we are talking about and a constructor \texttt{from\_record}\ednote{Update the name ; need to implement this w/ Florian} that takes a generic record and returns an member of this type. We dynamically introduce a set of virtual declarations into this theory: One for each record in the database. When it is requested, we check for all declared fields in the schema theory and use the appropriate codecs to construct an MMT record for the requested object. We then use the  \texttt{from\_record} constructor to define the MMT term corresponding to the record.

We also want to be able to access specific fields from the retrieved objects. For this we introduce a third theory that declares a set of accessors, that is a function from the type of records in the database to the appropriate types. Each accessor also contains meta-data that states which field from the schema theory it implements. With the help of dynamically generated rules we can then simplify terms by extracting the appropriate fields from the records that were used to construct them. \ednote{This currently does not work; why?}.

\ednote{Transition to the next section; missing}

\section{LMFDB as an Example Implementation}\label{sec:lmfdb}

\ednote{Write about the concrete lmfdb example}
\ednote{Give an actual example}
\ednote{Give an overview of the written theories}

\section{Outlook and Conclusion}\label{sec:conclusion}

In this report we introduced the basics of how we imagine MMT data-centric theories. Here we connect to external databases which we model as a set of well-typed records, that is list of (key, value) pairs. We introduced the concept of record types inside MMT. Here keys are symbols which are declared inside a schema theory and the values are MMT literals translated from the physical database representations using codecs.

Additionally we have implemented an an actual example. For this we used the lmfdb database of elliptic curves. We implemented a multitude of codecs that should also prove useful in future expansions of this implementation. We have demonstrated that it is possible to integrate databases inside MMT seamlessly so that it is not noticable that declarations are actually retrieved from a database instead of being declared from within MMT directly.

In the future we want to expand on this concept. Right now we can only translate database records into MMT objects. While we want to use the form of the objects used by MMT as the primary representation we want to be able to translate these objects to system specifc objects. Each system might have system-specific constructors and / or representations. In practice all systems will have a constructor for these objects. These will take a set of arguments. These arguments will either be primitive (in which case we can just encode them from MMT using a codec) or be complex objects themselves (in which case we can recurse into the entire procedure). Storing these encodings inside MMT we will be able to write thin interfaces to MMT, which can then easily retrieve objects from MMT in their prefered representation.

Together with the opposite process -- the understanding of objects by using accessors from arbitary systems -- will also allow (almost) arbitary systems to exchange objects via MMT. We are already working on a Python Client implementation. This will not apply any recoding to the objects -- it just retrieves records in an easily accessible form from MMT. In the future we are hoping to use this to integrate MMT and GAP and enable GAP to use any kind of object that MMT has access to.

\newpage\printbibliography
\ednote{Switch this back to bibtex?}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
