\section*{\texorpdfstring{Deliverable description, as taken from Github
issue
\href{https://github.com/OpenDreamKit/OpenDreamKit/issues/118}{\#118} on
2017-02-08}{Deliverable description, as taken from Github issue \#118 on 2017-02-08}}\label{deliverable-description-as-taken-from-github-issue-118-on-2017-02-08}

\begin{itemize}
\tightlist
\item
  \textbf{WP5:}
  \href{https://github.com/OpenDreamKit/OpenDreamKit/tree/master/WP5}{High
  Performance Mathematical Computing}
\item
  \textbf{Lead Institution:} University of Kaiserslautern
\item
  \textbf{Due:} 2017-02-28 (month 18)
\item
  \textbf{Nature:} Demonstrator
\end{itemize}

\section*{Report on superoptimizer and writing optimized assembly for
MPIR}\label{report-on-superoptimizer-and-writing-optimized-assembly-for-mpir}

\subsection{Problem statement}\label{problem-statement}

For this deliverable, our task was to write a superoptimizer which
tries\\
valid permutations (i.e., that do not change program behaviour) of\\
instructions in assembly functions, times each permutation, and
chooses\\
the fastest one.

New functions for recent processor architectures should be written,\\
making use of recently added features like AVX2 instructions, and\\
should be optimized with the super-optimizer where applicable.

\subsection{Work completed}\label{work-completed}

For the first six months of the project, we wrote the ``ajs''\\
superoptimizer (\url{https://github.com/akruppa/ajs}), based on\\
the open-source asmjit library (\url{https://github.com/asmjit/asmjit}).

For the second six months, we solved several problems with the ajs\\
superoptimizer, especially erratic timings that had put the concept\\
in jeopardy, and, with contributions from Jens Nurmann, wrote and/or\\
optimized a set of core functions for MPIR and some auxiliary
functions\\
used internally (see below).

\subsection{The ajs superoptimizer}\label{the-ajs-superoptimizer}

The biggest problem with the superoptimizer was the highly erratic
timings\\
it measured for function executions. This made it practically
impossible\\
to have it automatically choose (one of) the fastest permutations for
a\\
given function.

The major problem was that the RDTSC(P) instructions no longer count
cpu\\
core cycles, but cycles of a fixed-frequency counter, i.e., elapsed
natural\\
time. Due to extensive clock scaling features of recent cpus, the
measured\\
time depended much more on power saving decisions made by the cpu than
on\\
the (comparatively small) speedup by finding a good permutation. This
is\\
especially true as functions may have to be superoptimized in several\\
pieces, e.g., separately for lead-in, core loop, and lead-out, to
reduce\\
the search space so that decent permutations are found within
acceptable\\
time.

The solution we used was the RDPMC instruction which provides
low-latency\\
access to performance measurement counters, including the ``second
fixed-\\
function counter'' (FFC2) which does, in fact, count cpu core clock
cycles.\\
The problem was enabling access to this counter from user mode
applications,\\
which requires setting some bits in MSR/CR. Attempts to do so via
kernel\\
modules we wrote turned out unreliable as the kernel disabled the bits
again\\
(and my modules killed machines on multiple occasions).

Eventually an excellent solution to this problem was found in the
jevents\\
library of the pmu-tools
(\url{https://github.com/andikleen/pmu-tools/})\\
which provides an API to the perf subsystem of the Linux kernel. This\\
allows enabling RDPMC to read FFC2 without the kernel spuriously
disabling\\
it again.

The resulting timings within one program run were much more stable
than\\
before, usually resulting in the same cycle count for a given
function.\\
The timings still vary by 1 occasionally (very rarely 2 or more); we
have\\
tried to find the source of the remaining variance, but to no avail.

Another major source of error, but invariant within one
super-optimizer\\
run, was the alignment of the stack, which appears to be randomly
chosen\\
at program start. The writes to the stack (PUSH/CALL) on a function
call\\
could alias (mod 4096) with the measured function's input operands,\\
causing ``partial address alias'' stalls which inflated execution time
by\\
as much as 10 cycles. This problem was solved by forcing a particular\\
stack alignment.

Other problems that occurred within ajs and which were solved:

\begin{itemize}
\tightlist
\item
  Jump instruction were always encoded in long form by asmjit,
  changing\\
  instruction alignment compared to other assemblers. We now manually\\
  annotate those instructions that require long form; all other use
  short.\\
  This requires manual work to annotate and verify the resulting
  instruction\\
  encodings.
\item
  Allow new registers introduced with AVX2, and instructions with 4
  operands
\item
  Various fixes and extensions to asm parsing code
\end{itemize}

All in all, fixing the aforementioned problems in ajs consumed well over
2 months of time on\\
the project. The code to generate permutations that honour data
dependencies is quite\\
powerful; however, subtle interactions with the cpu hardware made it\\
very time-consuming to get nearly cycle-accurate timings as we required.

\subsection{Optimized functions for
MPIR}\label{optimized-functions-for-mpir}

For Haswell and Skylake, the following set of core functions was\\
re-implemented or existing code optimized to take advantage of the\\
respective micro-architecture:\\
add\_n, sub\_n, addmul\_1, submul\_1, addlsh1\_n,\\
sublsh1\_n, com\_n, copyi, copyd, rshift1,\\
lshift1, rshift, lshift, mul\_1, mul\_basecase.

The only AMD CPU to which we could gain access was a Bulldozer which
is\\
a fairly old and poorly designed microarchitecture; in particular,\\
new instruction set extensions like AVX2 are so slow on Bulldozer (and\\
Piledriver) that they are best avoided. This left little room for\\
optimization, and we opted not to write new code for this out-dated
cpu,\\
but to cherry-pick existing code that performs well.

We are very grateful to Jens Nurmann who contributed significant\\
amounts of code and expertise on AVX2 programming, to Brian\\
Gladman for porting the new code to the Microsoft Visual C build\\
system, and to William Stein for granting us access to a Bulldozer\\
machine.

\subsubsection{Haswell}\label{haswell}

For Haswell, new AVX2 versions of com\_n, copyd, copyi,\\
lshift, lshift1, rshift, rshift1 were written anew and\\
super-optimized.

The addmul\_1, submul\_1, mul\_1, mul\_basecase, and sqr\_basecase\\
functions for Haswell in the GMP library were copied as these are
extremely\\
well optimized already - we did not think we could produce better in
what\\
little time we had left. Attempts to super-optimize these functions did
not\\
find better code.

Existing add\_n, sub\_n, karaadd, karasub, hgcd2 functions\\
were modified for Haswell and super-optimized, while sumdiff\_n and\\
nsumdiff\_n were written anew.

To give a summary of the speedups obtained, we include here results
obtained\\
with the mpir\_bench program
(\url{https://github.com/akruppa/mpir_bench_two}).\\
Higher values are better (function executions per unit time);\\
the apparent slow-down for size \textless{} 512 GCD is to be
investigated.

\begin{longtable}[c]{@{}lll@{}}
\toprule
Program multiply (weight 1.00) & Old & New\tabularnewline
\midrule
\endhead
128 128 & 108222650 & 107111633\tabularnewline
512 512 & 22816149 & 26895874\tabularnewline
8192 8192 & 228124 & 289984\tabularnewline
131072 131072 & 3884 & 5015\tabularnewline
2097152 2097152 & 173 & 203\tabularnewline
128 128 & 108109328 & 107223557\tabularnewline
512 512 & 17689648 & 20384648\tabularnewline
8192 8192 & 155145 & 189057\tabularnewline
131072 131072 & 2771 & 3479\tabularnewline
2097152 2097152 & 118 & 133\tabularnewline
15000 10000 & 80120 & 91788\tabularnewline
20000 10000 & 61030 & 71776\tabularnewline
30000 10000 & 37966 & 42448\tabularnewline
16777216 512 & 501 & 658\tabularnewline
16777216 262144 & 24.6 & 28.7\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[c]{@{}lll@{}}
\toprule
Program gcd (weight 0.50) & Old & New\tabularnewline
\midrule
\endhead
128 128 & 3729465 & 3646816\tabularnewline
512 512 & 767983 & 554155\tabularnewline
8192 8192 & 10974 & 15908\tabularnewline
131072 131072 & 175 & 223\tabularnewline
1048576 1048576 & 9.38 & 11.5\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[c]{@{}lll@{}}
\toprule
Program gcdext (weight 0.50) & Old & New\tabularnewline
\midrule
\endhead
128 128 & 2628011 & 2036197\tabularnewline
512 512 & 595026 & 451973\tabularnewline
8192 8192 & 7900 & 11192\tabularnewline
131072 131072 & 129 & 171\tabularnewline
1048576 1048576 & 6.04 & 7.94\tabularnewline
\bottomrule
\end{longtable}

The new code can be found in the directory\\
\url{https://github.com/akruppa/mpir/tree/master/mpn/x86_64/haswell}

\subsubsection{Skylake}\label{skylake}

For Skylake, add\_n, sub\_n, mul\_1, add\_err1\_n and sub\_err1\_n were
written anew\\
and super-optimized.\\
The addmul\_1, mul\_basecase and sqr\_basecase functions were taken from
GMP.\\
The other functions for Haswell are used as fall-backs.

\begin{longtable}[c]{@{}lll@{}}
\toprule
Program multiply (weight 1.00) & Old & New\tabularnewline
\midrule
\endhead
128 128 & 123326551 & 123312872\tabularnewline
512 512 & 29477397 & 33899135\tabularnewline
8192 8192 & 298474 & 358841\tabularnewline
131072 131072 & 4924 & 6024\tabularnewline
2097152 2097152 & 213 & 246\tabularnewline
128 128 & 123340235 & 123340948\tabularnewline
512 512 & 22551903 & 25322713\tabularnewline
8192 8192 & 208058 & 238204\tabularnewline
131072 131072 & 3497 & 4316\tabularnewline
2097152 2097152 & 142 & 155\tabularnewline
15000 10000 & 104503 & 112647\tabularnewline
20000 10000 & 80121 & 89101\tabularnewline
30000 10000 & 47871 & 54247\tabularnewline
16777216 512 & 611 & 693\tabularnewline
16777216 262144 & 29.1 & 33.6\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[c]{@{}lll@{}}
\toprule
Program gcd (weight 0.50) & Old & New\tabularnewline
\midrule
\endhead
128 128 & 4387356 & 4373122\tabularnewline
512 512 & 814864 & 682194\tabularnewline
8192 8192 & 11468 & 18970\tabularnewline
131072 131072 & 208 & 274\tabularnewline
1048576 1048576 & 11.3 & 14.1\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[c]{@{}lll@{}}
\toprule
Program gcdext (weight 0.50) & Old & New\tabularnewline
\midrule
\endhead
128 128 & 2750101 & 2562046\tabularnewline
512 512 & 640358 & 557060\tabularnewline
8192 8192 & 8526 & 13743\tabularnewline
131072 131072 & 155 & 212\tabularnewline
1048576 1048576 & 7.50 & 9.83\tabularnewline
\bottomrule
\end{longtable}

The new code can be found in the directory\\
\url{https://github.com/akruppa/mpir/tree/master/mpn/x86_64/skylake}

\subsubsection{Bulldozer}\label{bulldozer}

On Bulldozer, the speed gains obtained are much more humble than on
Haswell\\
and Skylake, as relatively few functions were replaced by faster ones.\\
This microarchitecture is not a profitable target for code optimization
any\\
more.

It would have been very interesting to write optimized code for the new
Zen\\
microarchitecture, but as that was released only towards the end of
our\\
project, there was no opportunity to get access to such a machine.

\begin{longtable}[c]{@{}lll@{}}
\toprule
Program multiply (weight 1.00) & Old & New\tabularnewline
\midrule
\endhead
128 128 & 55322152 & 55550756\tabularnewline
512 512 & 12248577 & 12586138\tabularnewline
8192 8192 & 139406 & 138848\tabularnewline
131072 131072 & 2406 & 2421\tabularnewline
2097152 2097152 & 101 & 105\tabularnewline
128 128 & 55781257 & 51370568\tabularnewline
512 512 & 7690668 & 8710261\tabularnewline
8192 8192 & 90386 & 83592\tabularnewline
131072 131072 & 1587 & 1584\tabularnewline
2097152 2097152 & 64.0 & 65.9\tabularnewline
15000 10000 & 44703 & 45193\tabularnewline
20000 10000 & 33852 & 35294\tabularnewline
30000 10000 & 20000 & 20199\tabularnewline
16777216 512 & 268 & 294\tabularnewline
16777216 262144 & 12.7 & 13.4\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[c]{@{}lll@{}}
\toprule
Program gcd (weight 0.50) & Old & New\tabularnewline
\midrule
\endhead
128 128 & 2597029 & 2611829\tabularnewline
512 512 & 284031 & 289573\tabularnewline
8192 8192 & 6800 & 6810\tabularnewline
131072 131072 & 108 & 107\tabularnewline
1048576 1048576 & 5.77 & 5.77\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[c]{@{}lll@{}}
\toprule
Program gcdext (weight 0.50) & Old & New\tabularnewline
\midrule
\endhead
128 128 & 1270472 & 1239850\tabularnewline
512 512 & 223972 & 218197\tabularnewline
8192 8192 & 4944 & 4924\tabularnewline
131072 131072 & 78.1 & 78.0\tabularnewline
1048576 1048576 & 3.65 & 3.65\tabularnewline
\bottomrule
\end{longtable}

The new code can be found in the directory\\
\url{https://github.com/akruppa/mpir/tree/master/mpn/x86_64/bulldozer}

\subsubsection{Future work}\label{future-work}

The superoptimizer works reasonably reliably now and can be used to\\
optimize more functions in MPIR and other software projects.

The division and GCD functions in MPIR are worthwhile targets for\\
additional optimization work.

The new Zen microarchtecture of AMD looks promising for scientific\\
computation; an optimization effort here would be worthwhile.

\subsubsection{Source code}\label{source-code}

The ajs superoptimizer can be found at
\url{https://github.com/akruppa/ajs}\\
The optimized functions for MPIR are merged into the main MPIR\\
repository at \url{https://github.com/wbhart/mpir}
