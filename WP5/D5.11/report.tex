\documentclass{deliverablereport}
\usepackage{hyperref}

\deliverable{hpc}{sage-HPCcombi}
\deliverydate{31/08/2018}
\duedate{31/08/2018 (M36)}
\author{V. Delecroix, F. Hivert}

%\usepackage[nottoc, notlof, notlot]{tocbibind}
\usepackage[backend=bibtex]{biblatex}
\addbibresource{report.bib}
\makeatletter\def\blx@maxline{77}\makeatother

\usepackage{multirow}
\usepackage{xcolor,colortbl}
\usepackage{pgfplots}


\newcommand{\Cilk}{\texttt{Cilk}\xspace}
\newcommand{\CilkP}{\texttt{Cilk++}\xspace}
\newcommand{\CPP}{\texttt{C++}\xspace}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\sgnode}[1]{{\bf \left<#1\right>}}
\newcommand{\gr}[1]{{\color{gray} #1}}

\DeclareMathOperator{\Irr}{Irr}

\newtheorem{defi}{Definition}
\newtheorem{prop}{Proposition}


\setcounter{tocdepth}{1}
\begin{document}
\maketitle


\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Computer experimentations in discrete mathematics in particular
enumerative and algebraic combinatorics require high performance
computing. The search space are often huge and the best algorithmic
strategy for the exploration is not evident and depend often on
the answer to the question. Let us cite the following description
from~\cite{LoidlTrinder-Hecke}:
\begin{quote}{}
  Some discrete mathematical problems are embarrassingly parallel, and this
  has been exploited for years even at Internet scale, e. g. the “Great
  Internet Mersenne Prime Search”.  Many parallel algebraic computations
  exhibit high degrees of irregularity, at multiple levels, with numbers and
  sizes of tasks varying enormously (up to 5 orders of magnitude). They tend
  to use complex user-defined data structures, exhibit highly dynamic memory
  usage and complex control flow, often exploiting recursion. They make
  little, if any, use of floating-point operations.  This combination of
  characteristics means that symbolic computations are not well suited to
  conventional HPC paradigms with their emphasis on iteration over floating
  point arrays.
\end{quote}
This deliverable is about experimentations in combinatorics
that involve low-level optimization and parallelization as
well as the integration of these techniques in the computer
algebra system \Sage.

This deliverable has greatly beneficiated from the \ODK workshop
on \textit{Interfacing (math) software with low level libraries}
that was held in spring 2018 in Cernay-la-Ville (France).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Two examples from algebra}

To compare technologies, one needs to develop solution of some problem in
every tested technologies. It is therefore important to identify archetypal
problems which are both easy to implement and demanding on the technologies.
We first present two such problems:
\subsection{Counting and enumerating integer vectors}
Integer vectors, that is finite sequences of integers, are central
in combinatorics as they can be used to encode many different
objects. In each situation the integer vectors are subject to various
constraints and we will be interested in three different kind
\begin{itemize}
\item linear constraints (e.g. lower or upper bounds on the entries,
maximum difference between adjacent position, etc)
\item restriction on the content (e.g. each value should be used
at most once)
\item symmetries (e.g. lexicographically smallest among all possible
cyclic permutations).
\end{itemize}
One algorithmic problem combinatorics is trying to solve is to provide
efficient enumeration of integer vectors subject to these kind of
constraints.

For example, permutations of $\{1, \ldots, n\}$ are encoded by integer vectors
on $\{1, \ldots, n\}$ which contain exactly once each entry. In a sample
\Sage session the list can be obtained as follows
\begin{verbatim}
sage: P = Permutations(3)
sage: P.list()
[[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 1, 2], [3, 2, 1]]
\end{verbatim}
Other elementary examples include integer partitions
\begin{verbatim}
sage: Partitions(5).list()
[[5], [4, 1], [3, 2], [3, 1, 1], [2, 2, 1], [2, 1, 1, 1], [1, 1, 1, 1, 1]]
\end{verbatim}
or Lyndon words
\begin{verbatim}
sage: LyndonWords(2,4).list()
[word: 1112, word: 1122, word: 1222]
\end{verbatim}

Sometimes, one often just want to count the objects and possibly avoid
generating the list. In \Sage, access to this counting is very often
done by other means
\begin{verbatim}
sage: Permutations(30).cardinality()
265252859812191058636308480000000
sage: Partitions(1000).cardinality()
24061467864032622473692149727991
sage: LyndonWords(2, 120).cardinality()
11076899964874298931257370467884546
\end{verbatim}

In this deliverable we will demonstrate how the interface to the LaTTe package
in \Sage allows more efficient counting, how Cython has helped faster enumeration
in \Sage. We will also present some promising experimentations involving among
other things vectorization techniques (MMX, SSE and AVX instruction sets) and
shared memory multicore computing with \CilkP.

\subsection{Numerical semigroups}

The computation with numerical semigroup in known as Frobenius coin
problem. It ask for the largest monetary amount that cannot be obtained using
only coins of specified denominations. Fomally
\begin{defi}
  A \emph{numerical semigroup} $S$ is a subset of $\NN$ containing $0$, closed
  under addition and of finite complement in $\NN$.
\end{defi}
For example the set $S_E=\{0,3,6,7,9,10\}\cup\{x\in\NN, x\geq 12\}$
is a numerical semigroup. One of the challenging problem in the field of
semigroup is to understand how much numerical semigroups there are with some
given constraints. To state precisely the question we need a little
terminology.
\begin{defi}
  Let $S$ be a numerical semigroup. We call \emph{genus} of $S$ the
  cardinality of the complementary set $g(S)=\operatorname{card}(\NN\setminus
  S)$.
\end{defi}
For example the genus of the previous example $S_E$ is $6$, the cardinality of
$\{1,2,4,5,8,11\}$.

For a given positive integer $g$, the number of numerical semigroups of genus
$g$ is finite and is denoted by $n_g$. In J.A. Sloane's \emph{on-line
encyclopedia of integer sequences}~\cite{OEIS} we find the values of $n_g$
for $g\leq 52$. These values were obtained by M. Bras-Amor\'os
(\cite{BrasAmoros2008} for more details). 

Using adequate data structures and parallelization with \Cilk allowed
F.~Hivert with his collaborator J.~Fromentin to obtain the number of numerical
semigroups up to genus $g \leq 70$ and also confirm the so called
Wilf~\cite{Wilf} conjecture for $g \leq 60$. This work has led to the
publication~\cite{FromentinH16}.

Exploring this tree is quite challenging as a parallel problem. It involve
exploring a extremely large tree with $10^{15}$ node. Of course when exploring
such a tree, different branch can be explored in parallel. However if the tree
is unbalanced, we need a mechanism to rebalance the computation. It appears
that the tree of numerical semigroup is extremely unbalanced an is therefore a
good prototypical challenge of such kinds of computation. To give a few
figure: We compare nodes at depth $30$ anb $45$: The number of nodes at depth
30 and 45 are $5\,646\,773$ and $8\,888\,486\,816$. If we sort decreassingly
the number of descendants at depth $45$ of the nodes at depth $30$, then
\begin{itemize}
\item The first node has $42\%$~of the descendants;
\item The second one node has $7.5\%$~of the descendants;
\item The $1000$~first node have $99.4\%$~of the descendants;
\item Only $27\,321$ nodes have descendants at depth~$45$;
\item Only $257$ nodes have more than $10^6$~descendants;
\end{itemize}
We will discuss the different technologies to solve this problem.
\medskip

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Various classes of problems appearing often in algebra an combinatorics}

In our experiments, we have identified the following four kinds of problems:
\begin{itemize}
\item \emph{Embarrassingly parallel problems}: where little or no effort is
  needed to separate the problem into a number of parallel tasks.
\item \emph{Recursive parallel problems}: the computational problem is
  organized as a recursive tree which can be discovered on the fly during the
  computation.
\item \emph{Deep depended multi-task problems}: the computational problem is
  organized as a large set of task with a large graph of dependency.
\item \emph{Micro data structure parallel optimization}: many combinatorial
  structures (permutations, partitions, monomials, young tableaux), the data
  can be encoded as a small sequence of small integers that can often
  efficiently be handled thanks to vector instructions.
\end{itemize}

\section{An overview of techniques and technology}

Before entering into more depth into details of this deliverable it is
important to draw a general picture of the algorithmic techniques
and the hardware and software technologies available.

Let us first distinguish the following two families of programming
languages. On the one hand the interpreted languages
such as \Python or Julia. And on the other hand we have the compiled languages
such as C/C++. The former languages are very convenient for end-users as
they provide high level data structures and instructions as well as automatic
memory management. This is one of the reason of the choice for \Python as the
base language for \Sage. Though, these higher level languages suffer slowness,
especially when large iterations have to be performed. In this situation, it is
preferable to go in the details of memory allocation and CPU instructions.

A first ingredient of optimization that provides a bridge between interpreted
and compiled language is provided by Cython that is a Python
to C/C++ compiler. Cython plays an important role in this deliverable. It
was also central in the development of the cypari library, see deliverables
D4.1 and D4.10.

When one consider a low-level language such as C, one has on the first hand
access to CPU vectorized instructions, that is to say a single instruction that
is performed on multiple data (SIMD). These
instructions are also of critical importance for performant linear
algebra as implemented in the LinBox library. The relevance in the context
of combinatorics is discussed in Section~\ref{subsec:combi:SIMD}.

The second level of parallelization concerns multi-core computations. This can
be conveniently dealt with language extensions such as Open MP,
Threading Building Block (TBB) or Cilk. At this level one important
ingredient in optimization is the careful usage of shared memory.
Before entering into technical details, we describe in
Section~\ref{subsec:map-reduce:Sage} a small framework that was
implemented in \Sage that also illustrates the typical problem one faces
in combinatorics. More detailed on the usage of \Cilk are provided
in Sections~\ref{sec:low:level}.

The last level of parallelism concerns computations with multiple nodes
and is not addressed by this deliverable.

\subsection{Combinatorial structures and vector instructions}
\label{subsec:combi:SIMD}

SIMD instruction sets appeared
in three stages: MMX (set of single instruction multiple data instruction set
introduced in 1997), SSE (Streaming SIMD Extensions, introduced in 1999) and
more recently AVX (Advanced Vector Extensions, introduced in 2008). 

In many combinatorial structures (permutations, partitions, monomials, young
tableaux), the data can be encoded as a small sequence of small integers that
can often efficiently be handled thanks to vector instructions.  For example,
on the current \texttt{x86} machines, small permutations ($N\leq 16$) are very
well handled. Indeed thanks to machine instructions such as \verb+PSHUFB+ (Packed
Shuffle Bytes), applying a permutation on a vector only takes a few cycles.  Here
are some examples of operation with their typical speedups:
\[
\begin{tabular}{l|c}
Operation & Speedup \\\hline
Sum of a vector of bytes & $3.81$\\
Sorting a vector of bytes& $21.3$\\
Inverting a permutation& $1.97$\\
Number of cycles of a permutation& $41.5$\\
Number of inversions of a permutation& $9.39$\\
Cycle type of a permutation& $8.94$\\
\end{tabular}
\]
As a more concrete example, here is how to sort an array of $16$~bytes:
\begin{verbatim}
// Sorting network Knuth AoCP3 Fig. 51 p 229.
static const array<Perm16, 9> rounds =
    {{ { 1, 0, 3, 2, 5, 4, 7, 6, 9, 8,11,10,13,12,15,14},
       { 2, 3, 0, 1, 6, 7, 4, 5,10,11, 8, 9,14,15,12,13},
       [...]
    }};

Vect16 sort(Vect16 a) {
  for (Perm16 round : rounds) {
    Vect16 minab, maxab, blend, mask, b = a.permuted(round);
    mask = _mm_cmplt_epi8(round, Perm16::one);
    minab = _mm_min_epi8(a, b);
    maxab = _mm_max_epi8(a, b);
    a = _mm_blendv_epi8(minab, maxab, mask);
  }
  return a;
}
\end{verbatim}

Unfortunately from a user point of view, this requires rethinking all the
algorithms, and there is nearly no support by the compiler.\bigskip

So has part of the OpenDreamKit deliverable, we started to develop a new
library called \texttt{HPCombi} \url{https://github.com/hivert/HPCombi}. The
goal is to use SSE and AVX instruction sets for very fast manipulation of
combinatorial objects of small sizes. Is it still in a experimental stage and
currently deals only with permutation, transformation and partial
transformations. We have experimental code for partitions and boolean
matrices.

Despite its infancy the code is already used by LibSemigroup
\cite{libsemigroup} (which deals with a different kinds of semigroup than the
numerical one) by James Mitchell. It is a C++ library for semigroups and
monoids using C++11; The libsemigroups library is used in the Semigroups
package for GAP. The development version is available on Github, and there are
python bindings which makes it usable from Sage. The development of a more
thorough Sage interface is planned.

Finally, we want to stress out that this is actually a research
problem. Indeed, except for sorting a vector where we used a classical sorting
network algorithm, all the operation in the array above requires the design of
a new algorithm as no know algorithm where conceived with vector instructions
in minds. For example, for the simple task of inverting a permutation, we
designed 4 new different algorithms:
\begin{itemize}
\item \verb|inverse_sort|: which use a sort internally;
\item \verb|inverse_search|: which uses some kinds of parallel binary search. With
  the current size of vector registers ($16$~bytes), it is not the fastest one
  but it is the only of logarithmic complexity so that it should be the
  fastest once larger register will be available;
\item \verb|inverse_power|: which use a binary powering;
\item \verb|inverse_cycle|: which use the theoretical notion of cycle
  decomposition. It is currently the fastest.
\end{itemize}

\subsection{Map-reduce and its implementation in \Sage}
\label{subsec:map-reduce:Sage}

Map-reduce is a general programming model that is shared by many
parallelization problem and is relevant to our situation. In this section we
present a small framework implemented in Sagemath~\cite{sage} allowing
performance map/reduce like computations on
large recursively defined sets. Map-Reduce is a classical programming model
for distributed computations where one maps a function on a large data set and
uses a reduce function to summarize all the produced information. It has a
large range of intensive applications in combinatorics:
\begin{itemize}
  \item Compute the cardinality;
  \item More generally, compute any kind of generating series;
  \item Test a conjecture: i.e. find an element of $S$ satisfying a specific
    property, or check that all of them do;
  \item Count/list the elements of $S$ having this property.
\end{itemize}
Use cases in combinatorics often have two specificities: First of all, due to
combinatorial explosion, sets often don't fit in the computer's memory or
disks and are enumerated on the fly. Then, many problems are flat, leading to
embarassingly parallel computations which are easy to parallelize. However, a
second very common use case is to have data sets that are described by
recursion tree which may be heavily unbalanced (as with numerical semigroups
described in previous section).

The framework~\cite{map-reduce} we developed works on the following input:
A \textbf{recursively enumerated set} given by:
\begin{itemize}
\item the \texttt{roots} of the recursion
\item the \texttt{children} function computing
\item the \texttt{postprocessing} function that can also filter intermediate
  nodes
\end{itemize}
Then, a \textbf{Map/Reduce problem} is given by:
\begin{itemize}
\item the \texttt{mapped} function
\item the \verb|reduce_init| function
\item the \texttt{reduce} function
\end{itemize}
Here is an example where we count binary sequence of length 15:
\begin{verbatim}
sage: S = RecursivelyEnumeratedSet( [[]],
....:   lambda l: [l+[0], l+[1]] if len(l) <= 15 else [],
....:   post_process = lambda x : x if len(x) == 15 else None,
....:   structure='forest', enumeration='depth') 
sage: sage: S.map_reduce(
....:   map_function = lambda x: 1,
....:   reduce_function = lambda x,y: x+y,
....:   reduce_init = 0 )
32768
\end{verbatim}
This framework uses a multi-process implementation of a work-stealing
algorithm~\cite{BlumofeL99, BlumofeL99}, and scales relatively well, as shown
below in a typical computation:
\[\begin{tabular}{ccccc}
\# processors & 1 & 2 & 4 & 8 \\
Time (s) & 250& 161& 103 & 87 \\
\end{tabular}
\]

Though it doesn't really qualify as HPC, it allowed to efficiently
parallelize a dozen of experiments ranging from Coxeter group and
representation theory of monoids to the combinatorial study of the C3
linearization algorithm used to compute the method resolution order (MRO) in
scripting language such as \Python and Perl~\cite{C3-controled}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Low-level experimentations}
\label{sec:low:level}

% \subsection{Integer vectors up to permutations}
% \label{subec:integer:vectors}

% We briefly report a successful optimization using the technologies
% evoked in the previous section. The aim was to optimize an algorithm
% developed by N.~Borie for enumerating integer vector modulo permutation
% groups~\cite{Borie}.

% The problem is the following: we are given a subgroup $G$ of the symmetric
% group $S_n$. It acts by permutation of coordinates on the vectors in $\NN^n$.
% The problem is to generate one vector in each orbit. Note that there are
% infinitely many such vectors; in practice one usually wants to enumerate the
% vectors with a given sum or content. 

% N.~Borie designed a tree structure on those vectors which allows to enumerate
% them recursively. At the level of each node, a relatively complicated
% computation is done involving partial lexicographic comparison and a hash
% table to avoid some duplication. The goal was to optimize the particular case
% of small groups where $n\leq16$. The development went along the following
% steps:
% \begin{itemize}
% \item permutation, vectors and lexicographic comparison using vector
%   instructions;
% \item recursive enumeration using \CilkP
% \item used thread local strorage for the hash table at the level of each node
% \item designed a handmade hash table to avoid dynamic allocation and adapted
%   to the specific use-case
% \end{itemize}
% This last step is due to a very specific use case for the hash table: we
% needed it to store a dynamic set where we only add elements and never remove
% one, and we clear the hash table very often. Profiling showed that the hash
% table may grow up to thousand of elements but, on the average, is
% only cleared when containing $2.5$~elements ! We decided therefore to use a
% closed bounded hash table together with a linked list of used buckets to be
% able to clear the table quickly.

% Altogether, we compared our optimized version with an already optimized
% non-parallel compiled version using the \Python compiler Cython. Computing the
% $375810$~integer vectors of sum 25 for the largest transitive subgroup of
% $S_{16}$ took $9$min $23$s on a single core with Sage's code, whereas our code
% is able to do it in $0.503$s on $8$~cores for a speedup of $1112$ times.
% Finally, the code (not yet released) is downloadable at~\cite{IVMPG}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Numerical semigroups}

As we already explained, we picked up the computation of numerical semigroup
as a good challenge for large trees explorations. This is a classical
computational problem as there are various technologies to solve it. The idea
here is that different branches of the tree can be explored in parallel by
different cores of the computer. The tricky part is to ensure that all cores
are busy, giving a new branch when a core is done with a former one.

What makes combinatorics particular is the size of the trees (upto $10^{15}$
in our experiments and the granularity of the computation (less than $100ns$
is spent on each tree node). This is extremely demanding on the load balancing
algorithms and their implementation.

The clear algorithmic solution is to use a work stealing algorithms. They have
been described in \cite{10.1109/SFCS.1994.365680, BlumofeL99} and we made a
Python implementation with a map-reduce fronted in deliverable
5.1~\cite{map-reduce}. However, it was clear from the beginning that this
implementation was a tool to help rapid prototyping in a day-to-day research
use case but it wasn't meant as high performance computing. So we experimented
with a few low level technologies. We present them here together with their
behavior in our prototypical examples:
\begin{itemize}
\item The \CilkP~\cite{CilkIntel} technology is particularly well suited for
  those kinds of problems. For our computation, we used the free version which
  is integrated since version 5.8 of the GNU~C compiler~\cite{GCCcilk}.

  \Cilk is a general-purpose language designed for multithreaded parallel
  computing. The \CPP incarnation is called \CilkP. The biggest principle
  behind the design of the \Cilk language is that the programmer should be
  responsible for \emph{exposing} the parallelism, identifying elements that
  can safely be executed in parallel; the run-time environment decide during
  execution how to actually divide the work between cores. The parallel
  features of \CilkP are used mainly through the \texttt{cilk\_spawn} keyword:
  used on a procedure call, it indicates that the call can safely operate in
  parallel with the remaining code of the current function. Note that the
  scheduler is not obliged to run this procedure in parallel; the keyword
  merely alerts the scheduler that it can do so.

  Allover, this makes \CilkP very easy to use, which short and very readable
  source code. Moreover, it turn out that \CilkP is extremely good to solve
  those kind of problems.  To give some figure of the performance we managed
  to achieve, we performed a full exploration of the tree up to depth~$70$ on
  a $32$~Haswell core at $2.3$~Ghz. The number of monoid at depth $70$ is
  $1607394814170158$.  It tooks $2.528\cdot10^{6}~s$ ($29$~days and $6$~hours)
  exploring $2590899247785594=2.59\cdot10^{15}$ monoids at a rate of
  $1.02\cdot10^{9}$ monoids per second. Each monoid is stored in
  $240$~bytes. Storing all the computed monoids would take
  $6.22\cdot10^{17}$~bytes of data, which means that we generated
  $2.46\cdot10^{11}$~bytes of data per second.

  The main drawback which is huge is that both Intel and the GCC team decided
  to \textbf{deprecate and no longer maintain the \CilkP extension}. So we
  looked for alternatives.

\item The \emph{OpenMP} (Open Multi-Processing) is an application programming
  interface (API) that supports multi-platform shared memory multiprocessing
  programming in C, C++, and Fortran. It consists of a set of compiler
  directives, library routines, and environment variables that influence
  run-time behavior. It allows to spawn task using pragma directive on the
  compiler.

  Unfortunately for the kinds of computation like numerical semigroup, there
  is a huge number of small task that are spawned. We used the implementation
  from GCC compiler. At the time of our experiment, the finding was that the
  scheduler doesn't scale to this huge number of task. The overhead was
  several order of magnitude larger than \CilkP.

\item An indication that this is still a research level problem is that we
  have found that at the university of Glasgow there where a thesis exactly on
  this problem which moreover tried to distribute the computation on a cluster
  of machine. This these was made by Blair Archibald under the supervision of
  Phil Trinder. They developped a C++ library called YewPar
  \cite{YewPar} which they presented as ``A
  Collection of High Performance Parallel Skeletons for Tree Search Problems''
  using the well established HPX~\cite{HPX} parallel library/runtime.

  At the time we where in contact, they already had decided to use our
  numerical semigroup algorithm as a benchmark four their scheduling
  implementation, which we consider as a good support our choice. Though not
  as easy as \CilkP, it is easy to fit in one of YewPar skeleton, and we gain
  distribution of the algorithm on several machine. If only used on one
  multicore machine, YewPar is more or less only $2.5$ time slower than
  \CilkP. The main question is long term maintenance: the web site says ``
  This library is currently experimental and should be considered very
  unstable'' and being the work of a PhD student, there is no warranty that it
  still will be there in a few years.

\item We also advised three master student internship namely, Adrien Pavão,
  Thomas Foltête and Edgar Fournival to explore the Spark technology and the
  Go language. It turned out that these technology weren't fit for our needs
  but the work allowed to find a basic bug in GCC implementation of \CilkP
  \cite{gcc-bug-80038}.
\end{itemize}

As a conclusion for large combinatorial trees explorations our current
recommendation is to use \CilkP which is both very efficient and simple to use
and learn. We hope that the alternative will manage to get to similar
performances.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Polytopes and linear programming}

Constraint on integer vectors can often be thought as linear constraints.
For example, the partitions of $n$ are given by non-negative integer
vectors $(x_1, \ldots, x_n)$ in $\mathbb{R}^n$ so that $x_1 \geq x_2 \ldots x_n$.
In other words, some general linear programming techniques come into
play.

During the \ODK workshop \textit{Sage Days 84} held in Olot (Spain) in spring
2017 we improved \Sage capacities for polytope computations, including
integer point counting and linear optimization. Among other things
\begin{itemize}
\item We provided an interface to the LattE software that uses Barvinok
method to count integer points in polytope.
\item We developed a \Sage interface to the Polymake software that is
one of the reference software in the field.
\item We started the development of a C++-library for polytope computations
over number fields that gave rise to the library E-ANTIC~\cite{eantic-code}
 and its inclusion in Normaliz~\cite{normaliz-code}.
\end{itemize}
        
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}



\printbibliography

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

