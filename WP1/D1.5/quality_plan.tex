\subsection{Quality assurance plan}

\subsubsection{Deliverables quality: Quality Review Board}

The Quality Review Board is the Consortium Body that fosters best
possible quality in the delivered work of the project.
All four members of the board
have a research interest in the quality of software in computational
science, and use and share their experience to benefit the quality of
the work.

The board is chaired by Hans Fangohr, from the University of
Southampton and European XFEL GmbH. He is supported in this task by
Mike Croucher from the University of Leeds, Alexander Konovalov from
the University of St Andrews, and by Konrad Hinsen from the Centre de
Biophysique Moléculaire with whom a Non-Disclosure Agreement was
signed.

The members engage with European initiatives working towards
improvement of the software quality in research, in particular in
computational and data science; both as voluntary activities and key
of their professional roles. Mike Croucher is the head of research
computing at Leeds, well known through his outreach blog; Alexander Konovalov
is a fellow of the Software Sustainability Institute and an active
member of the Software Carpentry community; Konrad Hinsen has
founded and is editing the ReScience Journal for reproducible Science,
and Hans Fangohr is founder and director of the UK's only centre for
doctoral training in computational modelling, a fellow of the Software
Sustainability Institute, was chairing the EPSRC's national scientific
advisory committee on high performance computing, and is leading big
data analysis infrastructure development at the European XFEL research
facility.

The quality review board has reviewed deliverables at the end of
reporting period 1, identified good practice - both in terms of
software engineering content but also presentation of the work -,
produced a report, and shared the findings with all members in the
project to improve the quality of the remaining deliverables. A
summary is included in Sec.~\ref{sec:summ-recomm-deliv}. The
board has stuck to its no-blame culture in its reporting.

A similar process is underway for the end of reporting period 2.


\subsubsection{Good practice}

The quality review board notes that there is no firmly established
view on what best practice establishes, and that (i) we expect our
best practice checklist to grow and change, and (ii) that due to the
variety of possible outputs not all categories will be appropriate for
every item under review. Here is a summary of good practice, with
particular focus on software and computational projects.

\paragraph{Software engineering}
\label{sec:org2e9824e}

Use of
\begin{itemize}
\item[{$\square$}] version control
\item[{$\square$}] tests
\item[{$\square$}] automated tests
\item[{$\square$}] continuous integration
\item[{$\square$}] automatic building of releases
\end{itemize}

\paragraph{Dissemmination}
\label{sec:org1f65c9b}
\begin{itemize}
\item[{$\square$}] Host code publicly (Github, \ldots{})
\item[{$\square$}] Reference Manual (APIs)
\item[{$\square$}] Tutorial (for beginning users)
\item[{$\square$}] Examples
\item[{$\square$}] Offer live interactive online demos (for example
  through Binder)
\item[{$\square$}] Support mechanisms (email/forum/gitter/github issues/\ldots{})
\item[{$\square$}] How to cite the output?
\item[{$\square$}] Installation mechanism
\item[{$\square$}] High level description of tool/activity accessible to non-experts
\item[{$\square$}] URLs/Blog/etc to and from  OpenDreamKit project
\item[{$\square$}] Grant acknowledgements
\item[{$\square$}] Open Source license
\item[{$\square$}] Workshop
\item[{$\square$}] Engaging users
\end{itemize}

\paragraph{Pathways to impact}
\label{sec:orgc218a3a}
\begin{itemize}
\item[{$\square$}] Does the software address the needs of the users?
\item[{$\square$}] Workshops to gather feedback
\end{itemize}

\subsubsection{Summary of recommendations for deliverable reports}
\label{sec:summ-recomm-deliv}
For reports that are well written, the quality review board found good
software engineering practices. However, for some deliverables the
reports were more difficult to assess. To address this, the following
guidance has been developed:

\begin{itemize}
\item Context setting
  \begin{itemize}
  \item what is the problem that is being addressed?
  \item Why should we care about the deliverable?
  \item what is the thing that has been created?
  \end{itemize}

\item Stating the obvious: for example if people (outside the project)
  are excited about it

\item Provide introduction to topics, even for people not too familiar
  with the field
\item Comment on other good (software) practices you may use without
  thinking about it (version control, testing, continuous integration,
  distribution)

\item Comment on the testing that has been done, even if not
  automatic. If there is a prototype / demonstrator, explain it in
  more detail.
\item Work relating to sustainability, should be mentioned, even if
  implicit (for example growing a community through workshops will
  help to make the project more sustainable).

\item Anything with impact should be mentioned (contribution / uptake
  to Software carpentry, other computational science and software
  projects, github, users, . . . )

\item Have an executive summary on the
  first page (just half a page).
  \begin{itemize}
  \item why does this deliverable exist? (context)
  \item have you achieved everything you set out to do?
  \item what would be / are the next steps?
  \end{itemize}

\end{itemize}

%\ODK consortium about the deliverables content and layout.

% While the primary focus of the board is on the OpenDreamKit project
% and the software it develops,
% some of the lessons may be more widely applicable and be made publicly
% available.





% % original

% The content form of deliverables due by then had to meet the
% expectations of the Project Officer and of Reviewers. Following this
% experience, we have concluded that deliverables should be written in
% Latex using a style file created for this purpose. For deliverables
% that are not reports by themselves, it's appropriate to have a
% relatively short report with a link to the github issue, and a copy of
% the description of this issue. In all cases, the report shall be
% self-contained. Deliverables are indeed evaluated based upon their
% versions submitted on the EU portal without retrieving other
% resources. Links have no legal value, since there is no guarantee that
% the referenced material will remain unchanged.  Partners who have a
% deliverable due at month 12 (August 2016) have been following these
% tips. The feedback of the Official review will help the Quality review
% Board in ensuring the quality of reports meets the needs. This will be
% up to the Quality Review Board to meet after the 1st reporting period
% and to decide if the quality of deliverables is acceptable.  They will
% aim at identifying good practice and weaknesses, and to share the
% lessons with the project to improve any future project work. The board
% will focus on selected deliverables and investigate those in detail
% rather than attempting a superficial inspection of all deliverables.


\subsubsection{Infrastructure quality: End-user group}


It was decided by the Steering Committee during the
\href{http://opendreamkit.org/meetings/2015-09-02-Kickoff/management_structure/}{kick-off
  meeting} to slightly modify the management structure by having only
one gender-friendly Advisory Board composed of 6 people (as agreed a
few months later at the
\href{http://opendreamkit.org/meetings/2016-06-27-Bremen/minutes/}{Bremen
  meeting}), some of which to be end-users.

Members of the board are: Jacques Carette from the McMaster University, Istvan Csabai from the Eötvös University Budapest,
Françoise Genova from the Observatoire de Strasbourg, Konrad Hinsen from the Centre de Biophysique Moléculaire,
William Stein who is CEO of SageMath, Inc. (SME), and Paul Zimmermann from INRIA.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "report"
%%% End:
