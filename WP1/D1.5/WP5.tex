  \subsubsection{WorkPackage 5: High Performance Mathematical Computing}
  \label{hpc}
%Explain, task per task, the work carried out in WP during the reporting period giving details of the work carried out by each beneficiary involved.

  \TODO{Update for Reporting Period 2}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  \paragraph{Overview}

  Workpackage 5 is about the development of high performance computing tools in
  mathematical virtual research environments. It is addressed at the level
  of each kernel library composing the computational tools of the project (\Pari,
  \GAP, \Linbox, \MPIR, \Sage, \Singular, ...), and also at the level of interfacing and exposing
  core parallel features to higher level programming interfaces.

  Key results obtained over the period for WorkPackage 5 are the following:
  %% Only list deliverables produced in the reporting period
  \begin{compactitem}
  %% \item A closer integration of \Linbox in \Sage with improved reliability and
  %%   computing efficiency.
  \item A fine grain parallelisation of matrix Fast Fourier Transform code in
    \FLINT, delivering high and scalable performances.
  \item Parallelization of relation  Sieving code in FLINT.
  \item A new super-optimizer for vectorized assembly code and its
    exploitation to improve the performances of the MPIR code.
  \item A MapReduce framework implementation to parallelize huge (out of core) datasets from
    combinatorics presented as recursion trees.
  \item \Cython can now use \Pythran to compile and vectorize \Numpy code.
  \item A new Sun Grid Engine notebook spawner for \Jupyter to drive
    interractive computations on HPC clusters from a Jupyter notebook.
  \end{compactitem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Milestones}

\subparagraph{\longmilestoneref{hpc-prototype}}

\emph{“User story: Astrid wants to run compute intensive routines
    involving both dense linear algebra and combinatorics. She has
    access through a JupyterHub-based VRE to a high end multi-core
    machine which includes a vanilla \Sage installation. She
    automatically benefits from the HPC features of the underlying
    specialized libraries (\Linbox, ...). This is a proof of concept
    of the overall framework to integrate the HPC advances of
    specialized libraries into a general purpose VRE.
    %
    It will prepare the final integration of a broader set of such
    parallel features for the end of the project.”}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Tasks}

\subparagraph{\longtaskref{hpc}{hpc-pari}}

No deliverable is due for the reporting period.
The generic paralellization engine in the \Pari C-library
has reached its maturity. It is now part of the standard releases of \Pari
and~\longdelivref{hpc}{pari-hpc1}
will be delivered on time on month 24.

\subparagraph{\longtaskref{hpc}{hpc-gap}}

No deliverable is due for the evaluation period but steady progress was made on
Deliverable~\longdelivref{hpc}{GAP-HPC-report}. Over this period, eight releases were cut
incorporating contributions to Deliverable~\longdelivref{component-architecture}{hpc-configure}.

Another major direction of efforts is the HPC-GAP integration:
HPC-GAP is a fork of \GAP initiated during the \scienceproject project, which
enables multithreaded calculations. Now that HPC-GAP has reached
maturity, it's critical for its widespread use and long term
maintainability to merge it back into \GAP's master branch.
The first step towards this long-standing goal, which is at the core of
Task~\taskref{hpc}{hpc-gap}, is the major release of \GAP 4.9, published in 2018 and allowing compilation in HPC-GAP
compatibility mode. It comes together with the new manual book called ``HPC-GAP Reference Manual'',
which is also available online at \url{https://www.gap-system.org/Manuals/doc/hpc/chap0.html}.
HPC-GAP integration required a major
refactorisation of \GAP's build system
mainly developed by our external collaborator Max Horn (Giessen) into \GAP's master
branch during the Sage-GAP-Days-85 we organized in March 2017. 
An overview of the most important changes introduced in GAP 4.9.1,
with links to corresponding GitHub entries,
can be found in the \GAP manual: \url{https://www.gap-system.org/Manuals/doc/changes/chap2.html}.
Many \GAP packages also have been updated to work in \GAP 4.9 and make use of its new features.

\subparagraph{\longtaskref{hpc}{hpc-linbox}}

The deliverable~\longdelivref{hpc}{LinBox-algo} due for this reporting period is
delivered on time. The contribution, as detailed in the report is twofold:
\begin{compactitem}
\item Several algorithmic innovations have been produced: a new matrix invariant
  for rank profiles~\cite{DumPerSul:fcrpmgbd16}, a new symmetric matrix
  factorization algorithm and its high performance implementation has been
  proposed~\cite{DuPe18}; new representation and algorithms for quasiseparable
  matrices~\cite{Pernet:cqm16,PerSto:tsegqm17}, and new developments on 
  interactive certificates for the security of large scale distributed
  computations on unsafe
  resources~\cite{DumKalTho:lticmpdsm16,DumLucPer:cftearp17}. These results have been
  presented in the main venues in the domain:
  the international conference ISSAC'16-17-18 and the Journal of Symbolic computation.
\item Major software developments have happened in the LinBox ecosystem, increasing
  robustness, maintainability and introducing new computational features, such
  as the new symmetric factorization algorithm or a parallel triangular matrix
  inversion implementation. Not only have these library been closely integrated
  within \Sage, but the interface has been fully rethoughts thanks to a broader
  support of C++ in Cython.
\end{compactitem}


Deliverable~\longdelivref{hpc}{LinBox-distributed} is making good progress
  thanks to the work of our \ODK engineer Hongguang Zhu. Several code prototypes
  already implement a Chinese remainder based parallel rational solver on a distributed
  infrastructure. We are now working on improving the communication patterns,
  and on designing a parallel rational vector reconstruction algorithm, as it
  has now become the bottleneck in this computation.
  
  \subparagraph{\longtaskref{hpc}{hpc-singular}}

  The only deliverable under consideration for this reporting perido
  is~\longdelivref{hpc}{singular-polyarith}

Multivariate polynomials are represented in Singular using the sdmp format. While this data structure is generally amenable to parallelization, the implementation and some of the algorithms in Singular are not. Since the last update much work has been invested in updating the algorithms and data structures and making Singular polynomial arithmetic competitive with other systems. This work has been done in the Singular submodule Flint, whose code is available at \url{https://github.com/wbhart/flint2}. We also now support polynomial exponents of unlimited size with the three basic monomial orderings of lex, deglex, and degrevlex. Suggestions by colleagues in the HPC community including Bernard Parisse, Michael Monagan, Roman Pearce, and Micka\"el Gastineau have been invaluable.

The serial implementations of the operations of multiplication, division and GCD are complete in both the dense and sparse cases and the performance is competitive with other systems. The parallel implementation of multiplication is also complete with competitive performance as well. We are on track to have division and GCD parallelized and delivered on time.

We have also been able to parallelize polynomial root clustering, which is a
major engine in Singular that benefits from fine-grained
parallelization. Performance improvements continue to be made by Remi Imbach
(now at NYU following his ODK contract). The fully working implementation is
at \url{https://github.com/rimbach/Ccluster}.




  \subparagraph{\longtaskref{hpc}{hpc-mpir}}

  All deliverables for this task have been delivered at the previous
  reporting period.
  
  \subparagraph{\longtaskref{hpc}{hpc-combi}}

  The goal of this task is to use combinatorics as a source of challenges to
  experiment on various HPC techniques. In the first
  deliverable~\longdelivref{hpc}{sage-paral-tree}, we successfully implement a
  MapReduce programming model on large datasets described by a recursion
  tree, which are too big to fit in memory. After chasing around some bug on
  MacOS posix support, the code was integrated in Sage (Trac Ticket 13580) and
  presented at the "journée du groupe de travail LaMHA" at the Université Pierre et
  Marie Curie on November the 26th of 2016.

  Since it is written purely in Python, the code doesn't perform well when the
  computation in each node is short. A good technology for handling such
  situations with fine grain parallelism, seems to be the 
  \software{Cilk++} runtime, and we are currently doing experiment which goal is to have a two
  stage parallel computation, where \software{Cilk++} is doing the load balancing on a
  machine (shared memory) and Python the load balancing among machines. As a
  base for our experiment, we are working on the enumeration of numerical
  monoids; indeed, it is a very challenging problem as the explored recursion
  tree is extremely unbalanced. We are currently able to have a code
  generating and process 50Gio/s on a single 8 core i7 machine. When the
  problem of re-balancing the work among several machines will be solved we
  hope to be able to have throughput higher than 1To/s on a network of machines.
  To this end, we are also experimenting with the \texttt{Spark}
  technology for distributed computations.

  \subparagraph{\longtaskref{hpc}{pythran}}

Mathematical software, such as \Sage, intensively rely on the Python
language for its expressivity. In order to harness most of a CPU computing
efficiency, critical code in such interpreted languages need to be compiled into
C code. This is precisely what \Cython and \Pythran are offering. The former
supports a broad range that \Python constructs, while the latter focuses on optimizing
\Numpy constructs for linear algebra.
The purpose of this task is to:
\begin{enumerate}
\item\label{pythran:cython} Implement a convergence between these two compilers
\item\label{pythran:sage} Expose the new capacity to optimize \Numpy code to
  the developers and  users of~\Sage.
\end{enumerate}

Concerning target~\ref{pythran:cython}, the delivery of \longdelivref{hpc}{pythran-sage} successfully implements the
proposed convergence: \Cython is now able to delegate compilation tasks to
\Pythran whenever \Numpy code is detected. In practice, benchmarks show that
resulting code executes faster. For instance computing euclidean norm of a large
floating point vector is sped up by a factor of 2.5 without vectorization and
a factor 3.7 with AVX2 vectorization enabled.

As for target~\ref{pythran:sage}, a technical lock is that \Pythran's analysis
of \Python typing used to be too weak to support the breadth of Object Oriented
programming style of \Sage. Deliverable \longdelivref{hpc}{pythran-typing}
addressed this issue by strongly enhancing \Pythran's type inference system in two ways: first, the compiler now more accurately tracks the identifier
$\leftrightarrow$ value binding, which in turns makes it possible to generate strongly
typed code for a wider class of \Python kernels.  Second, an unsound type checker for
\Pythran has been developed. It provides human-readable error report when a type error is
detected at compile time, when a cryptic internal error was previously reported. Both
algorithms have been extensively detailed in separated blog posts and the resulting
implementation is part of the official \Pythran 0.8.0 release.

Further work on \Pythran's integration to \Sage and the exposition of
parallelism via \software{Cilk++} is still in progress and should be delivered in
\longdelivref{hpc}{cython-pythran-cilk} at M24.

  \subparagraph{\longtaskref{hpc}{hpc-jupyter}}
  
It is common for academic High Performance Computing (HPC) clusters to make
use of schedulers based on Sun Grid Engine with Son of Grid Engine as one of
the most popular. It is used, for example, on the institutional HPC systems
in the Universities of Sheffield and Manchester in the United Kingdom. It is also used
on the regional N8 HPC facility, a system shared by the eight most research
intensive universities in the North of England.

In deliverable~\longdelivref{hpc}{SGE-jupyter}, we have developed and demonstrated a Sun Grid Engine
notebook spawner for Project Jupyter, allowing users to easily access
Jupyter notebooks on HPC clusters directly from the web-browser. This
development allows users with no background in High Performance Computing to
easily migrate workflows from laptop to HPC cluster, allowing them to access
greater resources with no additional training required.
